name: lobe-chat-database
services:
  network-service:
    image: alpine
    container_name: lobe-network
    restart: always
    ports:
      - '${MINIO_PORT}:${MINIO_PORT}' # MinIO API
      - '9001:9001' # MinIO Console
      - '${CASDOOR_PORT}:${CASDOOR_PORT}' # Casdoor
      - '${LOBE_PORT}:3210' # LobeChat
      - '3000:3000' # Grafana
      - '4318:4318' # otel-collector HTTP
      - '4317:4317' # otel-collector gRPC
    command: tail -f /dev/null
    networks:
      - lobe-network

  postgresql:
    image: pgvector/pgvector:pg17
    container_name: lobe-postgres
    ports:
      - '5432:5432'
    volumes:
      - './data:/var/lib/postgresql/data'
    environment:
      - 'POSTGRES_DB=${LOBE_DB_NAME}'
      - 'POSTGRES_PASSWORD=${POSTGRES_PASSWORD}'
    healthcheck:
      test: ['CMD-SHELL', 'pg_isready -U postgres']
      interval: 5s
      timeout: 5s
      retries: 5
    restart: always
    networks:
      - lobe-network

  minio:
    image: minio/minio:RELEASE.2025-04-22T22-12-26Z
    container_name: lobe-minio
    network_mode: 'service:network-service'
    volumes:
      - './s3_data:/etc/minio/data'
    environment:
      - 'MINIO_API_CORS_ALLOW_ORIGIN=*'
    env_file:
      - .env
    restart: always
    entrypoint: >
      /bin/sh -c "
        minio server /etc/minio/data --address ':${MINIO_PORT}' --console-address ':9001' &
        MINIO_PID=\$!
        while ! curl -s http://localhost:${MINIO_PORT}/minio/health/live; do
          echo 'Waiting for MinIO to start...'
          sleep 1
        done
        sleep 5
        mc alias set myminio http://localhost:${MINIO_PORT} ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD}
        echo 'Creating bucket ${MINIO_LOBE_BUCKET}'
        mc mb myminio/${MINIO_LOBE_BUCKET}
        wait \$MINIO_PID
      "

  # version lock ref: https://github.com/lobehub/lobe-chat/pull/7331
  casdoor:
    image: casbin/casdoor:v2.13.0
    container_name: lobe-casdoor
    entrypoint: /bin/sh -c './server --createDatabase=true'
    network_mode: 'service:network-service'
    depends_on:
      postgresql:
        condition: service_healthy
    environment:
      httpport: ${CASDOOR_PORT}
      RUNNING_IN_DOCKER: 'true'
      driverName: 'postgres'
      dataSourceName: 'user=postgres password=${POSTGRES_PASSWORD} host=postgresql port=5432 sslmode=disable dbname=casdoor'
      runmode: 'dev'
    volumes:
      - ./init_data.json:/init_data.json
    env_file:
      - .env

  searxng:
    image: searxng/searxng
    container_name: lobe-searxng
    volumes:
      - './searxng-settings.yml:/etc/searxng/settings.yml'
    environment:
      - 'SEARXNG_SETTINGS_FILE=/etc/searxng/settings.yml'
    restart: always
    networks:
      - lobe-network
    env_file:
      - .env

  lobe:
    image: lobehub/lobe-chat-database
    container_name: lobe-chat
    network_mode: 'service:network-service'
    depends_on:
      postgresql:
        condition: service_healthy
      network-service:
        condition: service_started
      minio:
        condition: service_started
      casdoor:
        condition: service_started

    environment:
      - 'NEXT_AUTH_SSO_PROVIDERS=casdoor'
      - 'KEY_VAULTS_SECRET=Kix2wcUONd4CX51E/ZPAd36BqM4wzJgKjPtz2sGztqQ='
      - 'NEXT_AUTH_SECRET=NX2kaPE923dt6BL2U8e9oSre5RfoT7hg'
      - 'DATABASE_URL=postgresql://postgres:${POSTGRES_PASSWORD}@postgresql:5432/${LOBE_DB_NAME}'
      - 'S3_BUCKET=${MINIO_LOBE_BUCKET}'
      - 'S3_ENABLE_PATH_STYLE=1'
      - 'S3_ACCESS_KEY=${MINIO_ROOT_USER}'
      - 'S3_ACCESS_KEY_ID=${MINIO_ROOT_USER}'
      - 'S3_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD}'
      - 'LLM_VISION_IMAGE_USE_BASE64=1'
      - 'S3_SET_ACL=0'
      - 'SEARXNG_URL=http://searxng:8080'
    env_file:
      - .env
    restart: always
    entrypoint: >
      /bin/sh -c "
        /bin/node /app/startServer.js &
        LOBE_PID=\$!
        sleep 3
        if [ $(wget --timeout=5 --spider --server-response ${AUTH_CASDOOR_ISSUER}/.well-known/openid-configuration 2>&1 | grep -c 'HTTP/1.1 200 OK') -eq 0 ]; then
          echo '⚠️Warning: Unable to fetch OIDC configuration from Casdoor'
          echo 'Request URL: ${AUTH_CASDOOR_ISSUER}/.well-known/openid-configuration'
          echo 'Read more at: https://lobehub.com/docs/self-hosting/server-database/docker-compose#necessary-configuration'
          echo ''
          echo '⚠️注意：无法从 Casdoor 获取 OIDC 配置'
          echo '请求 URL: ${AUTH_CASDOOR_ISSUER}/.well-known/openid-configuration'
          echo '了解更多：https://lobehub.com/zh/docs/self-hosting/server-database/docker-compose#necessary-configuration'
          echo ''
        else
          if ! wget -O - --timeout=5 ${AUTH_CASDOOR_ISSUER}/.well-known/openid-configuration 2>&1 | grep 'issuer' | grep ${AUTH_CASDOOR_ISSUER}; then
            printf '❌Error: The Auth issuer is conflict, Issuer in OIDC configuration is: %s' \$(wget -O - --timeout=5 ${AUTH_CASDOOR_ISSUER}/.well-known/openid-configuration 2>&1 | grep -E 'issuer.*' | awk -F '\"' '{print \$4}')
            echo ' , but the issuer in .env file is: ${AUTH_CASDOOR_ISSUER} '
            echo 'Request URL: ${AUTH_CASDOOR_ISSUER}/.well-known/openid-configuration'
            echo 'Read more at: https://lobehub.com/docs/self-hosting/server-database/docker-compose#necessary-configuration'
            echo ''
            printf '❌错误：Auth 的 issuer 冲突，OIDC 配置中的 issuer 是：%s' \$(wget -O - --timeout=5 ${AUTH_CASDOOR_ISSUER}/.well-known/openid-configuration 2>&1 | grep -E 'issuer.*' | awk -F '\"' '{print \$4}')
            echo ' , 但 .env 文件中的 issuer 是：${AUTH_CASDOOR_ISSUER} '
            echo '请求 URL: ${AUTH_CASDOOR_ISSUER}/.well-known/openid-configuration'
            echo '了解更多：https://lobehub.com/zh/docs/self-hosting/server-database/docker-compose#necessary-configuration'
            echo ''
          fi
        fi
        if [ $(wget --timeout=5 --spider --server-response ${S3_ENDPOINT}/minio/health/live 2>&1 | grep -c 'HTTP/1.1 200 OK') -eq 0 ]; then
          echo '⚠️Warning: Unable to fetch MinIO health status'
          echo 'Request URL: ${S3_ENDPOINT}/minio/health/live'
          echo 'Read more at: https://lobehub.com/docs/self-hosting/server-database/docker-compose#necessary-configuration'
          echo ''
          echo '⚠️注意：无法获取 MinIO 健康状态'
          echo '请求 URL: ${S3_ENDPOINT}/minio/health/live'
          echo '了解更多：https://lobehub.com/zh/docs/self-hosting/server-database/docker-compose#necessary-configuration'
          echo ''
        fi
        wait \$LOBE_PID
      "

  grafana:
    profiles:
      - otel
    image: grafana/grafana:12.2.0-17419259409
    container_name: lobe-grafana
    network_mode: 'service:network-service'
    restart: always
    environment:
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
      - GF_AUTH_DISABLE_LOGIN_FORM=true
      - GF_FEATURE_TOGGLES_ENABLE=traceqlEditor
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./grafana/datasources:/etc/grafana/provisioning/datasources
    depends_on:
      - tempo
      - prometheus

  tempo:
    profiles:
      - otel
    image: grafana/tempo:latest
    container_name: lobe-tempo
    network_mode: 'service:network-service'
    restart: always
    volumes:
      - ./tempo/tempo.yaml:/etc/tempo.yaml
      - tempo_data:/var/tempo
    command: ['-config.file=/etc/tempo.yaml']

  prometheus:
    profiles:
      - otel
    image: prom/prometheus
    container_name: lobe-prometheus
    network_mode: 'service:network-service'
    restart: always
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--web.enable-otlp-receiver'
      - '--web.enable-remote-write-receiver'
      - '--enable-feature=exemplar-storage'

  otel-collector:
    profiles:
      - otel
    image: otel/opentelemetry-collector
    container_name: lobe-otel-collector
    network_mode: 'service:network-service'
    restart: always
    volumes:
      - ./otel-collector/collector-config.yaml:/etc/otelcol/config.yaml
    command: ['--config', '/etc/otelcol/config.yaml']
    depends_on:
      - tempo
      - prometheus

  otel-tracing-test:
    profiles:
      - otel-test
    image: ghcr.io/grafana/xk6-client-tracing:v0.0.7
    container_name: lobe-otel-tracing-test
    network_mode: 'service:network-service'
    restart: always
    environment:
      - ENDPOINT=127.0.0.1:4317

volumes:
  data:
    driver: local
  s3_data:
    driver: local
  grafana_data:
    driver: local
  tempo_data:
    driver: local
  prometheus_data:
    driver: local


networks:
  lobe-network:
    driver: bridge
